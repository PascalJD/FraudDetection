{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détection de fraudes de cartes de crédit\n",
    "\n",
    "## Introduction\n",
    "\n",
    "L'objectif du projet consiste en l'élaboration d'algorithmes de classification capables de détecter les transactions frauduleuses dans un dataset kaggle de transactions de cartes de crédit. Les données peuvent être trouvées [ici](https://www.kaggle.com/mlg-ulb/creditcardfraud).\n",
    "\n",
    "Les cas de fraudes ne représentant que 0.173% du nombre total de transactions, l’implémentation des modèles et les métriques de mesures de performance utilisées doivent être adaptées aux données fortement débalancées. Il est également souhaitable que la classification soit sensible, c’est-à-dire que le taux des faux positifs soit minimal, puisque l'institution bancaire ne veut pas déranger inutilement ses clients.\n",
    "\n",
    "\n",
    "## Plan\n",
    "\n",
    "* Exploration des données\n",
    "  * Métriques\n",
    "* Prétraitement\n",
    "  * Normalisation\n",
    "* Méthodologie\n",
    "  * Ensembles d'entraînement et de test origniaux\n",
    "* Régression logistique\n",
    "  * Sélection des hyperparamètres\n",
    "  * Évaluation\n",
    "* Undersampling aléatoire\n",
    "  * Sélection de modèle\n",
    "  * Évaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time as t\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Resampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données\n",
    "\n",
    "Le dataset contient 284 807 exemples ayant chacun 31 traits caractéristiques numériques dont le temps et le montant de la transaction. Les 28 autres proviennent d'une analyse en composante principale (PCA) effectuée pour anonymiser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/creditcard.csv\")\n",
    "print('shape: ', df.shape)\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les exemples non-frauduleux sont étiquetés par la classe 0 et les exemples frauduleux par la classe 1. Seulement 492 (0.173%) des exemples sont de la classe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fraudes: 492\n",
      "Taux de fraudes: 0.173 %\n"
     ]
    }
   ],
   "source": [
    "fraud_count = df['Class'].value_counts()[1]\n",
    "fraud_ratio = np.around(fraud_count/len(df)*100, 3)\n",
    "print(\"Nombre de fraudes:\", fraud_count)\n",
    "print(\"Taux de fraudes:\", fraud_ratio, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Métriques\n",
    "\n",
    "Les données sont extrèmements débalancées. Le classifieur risque de mémoriser que les données sont débalancées et d'assumer que la majorité des exemples sont des cas non-frauduleux (sur-apprentissage) plutôt que de détecter les liens dans les traits caractéristiques permettant réellement de déterminer si une transaction est une fraude.\n",
    "\n",
    "Les cas de fraudes ne représentant que 0.172% du nombre total de transactions, l’implémentation des modèles et les métriques de mesures de performance utilisées doivent être adaptées aux données fortement débalancées. En effet, le taux de classifications (accuracy) n'est pas une mesure de performance naturelle lorsque les données sont débalancées. Pour l'illustrer, considérons un classifieur qui prédit la classe '0' à tous les nouveaux exemples de tests. Un tel classifieur obtiendrait un taux de classifications correctes de 99.827% sans détecter aucune fraude. Un tel classifieur est incapable de généraliser, même si son taux de classifications est élevé.\n",
    "\n",
    "D'autres métriques sont plus naturelles pour évaluer la performance d'un dataset débalancé. \n",
    "\n",
    "Soit $N$ le nombre total d'exemples.\n",
    "\n",
    "vrais positifs (VP): \n",
    "\n",
    "faux positifs (FP): \n",
    "\n",
    "vrais négatifs (VN):\n",
    "\n",
    "faux négaitfs (FN):\n",
    "\n",
    "Accuracy $:=\\frac{VP+VN}{N}$\n",
    "\n",
    "Precision $:=\\frac{VP}{VP+FP}$\n",
    "\n",
    "Recall $:=\\frac{VP}{VP+FN}$\n",
    "\n",
    "f1-score $:=2\\ \\frac{\\text{precision}\\ \\times\\ \\text{recall}}{\\text{precision}\\ +\\ \\text{recall}}$\n",
    "\n",
    "Support:\n",
    "\n",
    "Courbe ROC: Courbe de VP en fonction de FP pour un classifieur binaire.\n",
    "\n",
    "Roc Auc Score: aire sous la courbe ROC.\n",
    "\n",
    "Le recall nous est particulièrement intéressant comme il s'agit le taux de fraudes détectées.\n",
    "\n",
    "\"Many machine-learning techniques, such as neural networks, make more reliable predictions from being trained with balanced data. Certain analytical methods, however, notably linear regression and logistic regression, do not benefit from a balancing approach.\" (Wikipedia)\n",
    "\n",
    "Dotons-nous d'une fonction pour imprimer ces métriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_pred, title=''):\n",
    "    print(title,'\\n')\n",
    "    print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "    confusion_m = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_m.ravel()\n",
    "    \n",
    "    print('\\ntrue positives\\tfalse positives\\tfalse negatives\\ttrue negatives')\n",
    "    print('%.0f\\t\\t%.0f\\t\\t%.0f\\t\\t%.0f'% (tn, fp, fn, tp))\n",
    "\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, beta=1.0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    f1 = metrics[2]\n",
    "    support = metrics[3]\n",
    "    print('\\nClass\\t\\t 0\\t\\t1')\n",
    "    print('Precision\\t', '%.2f\\t\\t%.2f'% (precision[0], precision[1]))\n",
    "    print('recall\\t\\t', '%.2f\\t\\t%.2f'% (recall[0], recall[1]))\n",
    "    print('f1\\t\\t', '%.2f\\t\\t%.2f'% (f1[0], f1[1]))\n",
    "    print('support\\t\\t', '%.2f\\t%.2f'% (support[0], support[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitement\n",
    "\n",
    "### Normalisation\n",
    "On voudrait normaliser les colonnes Time et Amount comme pour les colonnes V1 à V28. Le but principal de cette normalisation est d'aider les techniques de convergences utilisées pour l'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_amount</th>\n",
       "      <th>norm_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "      <td>-1.996583</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "      <td>-1.996583</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "      <td>-1.996541</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   norm_amount  norm_time        V1        V2        V3        V4        V5  \\\n",
       "0     0.244964  -1.996583 -1.359807 -0.072781  2.536347  1.378155 -0.338321   \n",
       "1    -0.342475  -1.996583  1.191857  0.266151  0.166480  0.448154  0.060018   \n",
       "2     1.160686  -1.996562 -1.358354 -1.340163  1.773209  0.379780 -0.503198   \n",
       "3     0.140534  -1.996562 -0.966272 -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4    -0.073403  -1.996541 -1.158233  0.877737  1.548718  0.403034 -0.407193   \n",
       "\n",
       "         V6        V7        V8  ...       V20       V21       V22       V23  \\\n",
       "0  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "time = df['Time'].values.reshape(-1, 1)\n",
    "amount = df['Amount'].values.reshape(-1, 1)\n",
    "norm_time = scaler.fit_transform(time)\n",
    "norm_amount = scaler.fit_transform(amount)\n",
    "df.insert(0, 'norm_time', norm_time)\n",
    "df.insert(0, 'norm_amount',norm_amount)\n",
    "df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodologie\n",
    "\n",
    "On doit s'assurer d'utiliser convenablement les méthodes de resample puisqu'elles modifient la distribution des données.\n",
    "\n",
    "Premièrement, on sépare les données originales en ensembles d'entraînement et de test. Ensuite, pour la sélection du modèle, on utilise la validation croisée pour tester les différentes combinaisons de paramètres pour chaque modèle. Une fois un modèle choisi, on estime sa performance de généralisation sur l'ensemble de test original dont les exemples sont inconnus au prédicteur. L'estimation de la performance est donc non biaisée.\n",
    "\n",
    "Dans la phase de sélection du modèle des méthodes de resample, il est important de resample durant la validation croisée. Sinon on modifie forcément l'ensemble de validation et le prédicteur est optimisé pour des données resamplées. Le resample ne doit qu'agir sur l'entraînement pour que l'ensemble de validation corresponde à la vraie distribution.\n",
    "\n",
    "### Ensembles d'entraînement et de test origniaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio de fraudes (Train): 0.17254870488152324\n",
      "Ratio de fraudes (Test): 0.17321489179921118\n"
     ]
    }
   ],
   "source": [
    "df.sample(random_state=0)\n",
    "\n",
    "inputs, outputs = df.drop('Class', axis=1), df['Class']\n",
    "\n",
    "# If stratify not None, data is split in a stratified fashion, using this as the class labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.3, random_state=0, stratify=outputs)\n",
    "\n",
    "# Verify if distributions are the same\n",
    "print('Ratio de fraudes (Train):', y_train.value_counts()[1]/y_train.shape[0] * 100)\n",
    "print('Ratio de fraudes (Test):', y_test.value_counts()[1]/y_test.shape[0] * 100)\n",
    "\n",
    "# Create a new dataframe for training set\n",
    "train_df = X_train.copy()\n",
    "train_df['Class'] = y_train\n",
    "train_df.sample(random_state=0)\n",
    "\n",
    "X_train, y_train = X_train.values, y_train.values\n",
    "X_test, y_test = X_test.values, y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut définir une fonctionalité qui permet de faire un grid-search sur un dictionnaire de paramètres tout en resamplant l'ensemble d'entraînement durant la validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics['accuracy'] = accuracy_score\n",
    "metrics['recall'] = recall_score\n",
    "\n",
    "def grid_search_cv_resample(X_train, y_train, model, params, cv=5, resampler=None, metric='accuracy', verbose=True):\n",
    "    \"\"\"\n",
    "    Returns the best model found by cross validation.\n",
    "    \"\"\"\n",
    "    # Folds are made by preserving the percentage of samples for each class.\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=False)\n",
    "\n",
    "    param_grid = ParameterGrid(params)\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    \n",
    "    scoring = metrics[metric]\n",
    "    \n",
    "    time1 = t()\n",
    "    for param_combination in param_grid:\n",
    "        \n",
    "        # Set params\n",
    "        model.set_params(**param_combination)\n",
    "        \n",
    "        scores = []\n",
    "        accuracy = []\n",
    "        \n",
    "        # Cross validation\n",
    "        for train_index, val_index in skf.split(X_train, y_train):\n",
    "            \n",
    "            # Train-val split\n",
    "            X_val, y_val = X_train[val_index], y_train[val_index] \n",
    "            X_train_resampler, y_train_resampler = X_train[train_index], y_train[train_index]\n",
    "            # Resample training set\n",
    "            if resampler is not None:\n",
    "                X_train_resampler, y_train_resampler = resampler.fit_resample(X_train_resampler, y_train_resampler) # Shuffle this ?! \n",
    "            \n",
    "            # Train - predict on validation\n",
    "            model.fit(X_train_resampler, y_train_resampler)\n",
    "            pred = model.predict(X_val)\n",
    "            \n",
    "            # Evaluate performance\n",
    "            scores.append(scoring(y_val, pred))\n",
    "\n",
    "        score = np.mean(scores)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score, best_params = score, param_combination\n",
    "\n",
    "    time2 = t()\n",
    "    if verbose:\n",
    "        print('\\nBest params', best_params)\n",
    "        print(f'{metric} score:', best_score)\n",
    "        print('Time:', time2-time1)\n",
    "    \n",
    "    return model.set_params(**best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_resample(X, y, model, cv=5, resampler=None):\n",
    "    \n",
    "    # Folds are made by preserving the percentage of samples for each class.\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=False)\n",
    "    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    support = []\n",
    "    accuracy = []\n",
    "    \n",
    "    # Cross validation\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "\n",
    "        # Train-val split\n",
    "        X_val, y_val = X[val_index], y[val_index] \n",
    "        X_train_resampler, y_train_resampler = X[train_index], y[train_index]\n",
    "        # Resample training set\n",
    "        if resampler is not None:\n",
    "            X_train_resampler, y_train_resampler = resampler.fit_resample(X_train_resampler, y_train_resampler) # Shuffle this ?! \n",
    "\n",
    "        # Train - predict on validation\n",
    "        model.fit(X_train_resampler, y_train_resampler)\n",
    "        pred = model.predict(X_val)\n",
    "\n",
    "        # Evaluate performance\n",
    "        metrics = precision_recall_fscore_support(y_val, pred, beta=1.0)\n",
    "        precision.append(metrics[0])\n",
    "        recall.append(metrics[1])\n",
    "        f1.append(metrics[2])\n",
    "        support.append(metrics[3])\n",
    "        accuracy.append(accuracy_score(y_val, pred))\n",
    "    \n",
    "    precision = np.mean(precision, axis=0)\n",
    "    recall = np.mean(recall, axis=0)\n",
    "    f1 = np.mean(f1, axis=0)\n",
    "    support = np.mean(support, axis=0)\n",
    "    accuracy = np.mean(accuracy, axis=0)\n",
    "    \n",
    "    print('\\nAccuracy\\t', accuracy)\n",
    "    print('\\nClass\\t\\t 0\\t\\t1')\n",
    "    print('Precision\\t', '%.3f\\t\\t%.3f'% (precision[0], precision[1]))\n",
    "    print('Recall\\t\\t', '%.3f\\t\\t%.3f'% (recall[0], recall[1]))\n",
    "    print('f1\\t\\t', '%.3f\\t\\t%.3f'% (f1[0], f1[1]))\n",
    "    print('Support\\t\\t', '%.3f\\t%.3f'% (support[0], support[1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression logistique \n",
    "\n",
    "Pour commencer, essayons d'implémenter naïvement un classifieur simple comme la régression logistique sur la totalité des données. On utilise le taux de bonnes classifications (accuracy) comme mesure de performance.\n",
    "\n",
    "Comme discutter précédemment, on s'attend à ce que le modèle mémorise à tord le débalancement dans les données et qu'il éprouve des difficultées à voir ce qui caractérise vraiment une fraude. Il devrait prédire la classe 0 trop souvent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:   44.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres: {'C': 10, 'penalty': 'l2'}\n",
      "Meilleur résultat (moyenne des résultats de validation croisée): 0.9992375756661026\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Tuning penalty and regularization using cross validation (5 folds)\n",
    "params = {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "log_reg_gs= GridSearchCV(log_reg, params, scoring='accuracy', cv=5, verbose=1) # Using accuracy as scoring metric \n",
    "log_reg_gs.fit(X_train, y_train)\n",
    "print('Meilleurs paramètres:', log_reg_gs.best_params_)\n",
    "print('Meilleur résultat (moyenne des résultats de validation croisée):', log_reg_gs.best_score_)\n",
    "log_reg_best = log_reg_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation\n",
    "\n",
    "On évalue les performances sur l'ensemble de test pour estimer le pouvoir de généralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Régression logistique simple \n",
      "\n",
      "Accuracy: 0.999204147794436\n",
      "\n",
      "true positives\tfalse positives\tfalse negatives\ttrue negatives\n",
      "85282\t\t13\t\t55\t\t93\n",
      "\n",
      "Class\t\t 0\t\t1\n",
      "Precision\t 1.00\t\t0.88\n",
      "recall\t\t 1.00\t\t0.63\n",
      "f1\t\t 1.00\t\t0.73\n",
      "support\t\t 85295.00\t148.00\n"
     ]
    }
   ],
   "source": [
    "log_reg_best.fit(X_train, y_train)\n",
    "predictions = log_reg_best.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, predictions, 'Régression logistique simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que le taux de bonnes classifications semble élevé, le prédicteur n'a su que détecter 0,63% des fraudes. Ceci confirme notre hypothèse. On voudrait augmenter significativement cette quantité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling aléatoire\n",
    "\n",
    "L'idée est d'entraîner un modèle sur un ensemble de données balancé tel quel 50% des données sont des fraudes. Modifier ainsi les données d'entraînement introduit un biais qui sert à compenser le risque de surapprentissage dû au fléau des données débalancées.\n",
    "\n",
    "On sélectionne aléatoirement des données de fraudes et on fait un nouveau dataframe sur lequel on entraîne les modèles. On testera ensuite la performance du modèle le plus performant sur l'ensemble de test original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_under_sampler = RandomUnderSampler(sampling_strategy=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque une augmentation des variances pour les paramètres V1 à V28, cette variabilité est due au fait que notre ensemble d'entraînement undersampled est petit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection de modèle\n",
    "\n",
    "Peut-être devrait-on choisir une classe de fonction relativement peu expressive (basse capacité) parce que notre dataset n'a que 688 exemples, ce qui signifie qu'il a une grande variabilité. Un algorithme trop expressif risque de mémoriser le bruit et de surapprendre.\n",
    "\n",
    "Comme l'ensemble de données du undersample aléatoire est petit, le bruit et les données aberrantes ont un poids significatif. Par conséquent, les prédicteurs sont sujets au surapprentissage s'ils mémorisent ce bruit. C'est d'autant plus vrai pour la régression logistique qui est sensible aux exemples extrêmes. \n",
    "\n",
    "Pour augmenter le taux de fraudes détectées, on utilise la métrique recall pour sélectionner le modèle.\n",
    "\n",
    "Essayons quelques modèles linéaires comme la régression logistique et la machine à vecteurs de support ainsi que KNN (un peu plus expressif)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params {'C': 0.001, 'penalty': 'l2'}\n",
      "accuracy score: 0.9987510293768187\n",
      "Time: 8.376923322677612\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Tuning penalty and regularization using cross validation\n",
    "params = [{'penalty': ['l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}, {'penalty': ['l1'], 'solver': ['liblinear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "log_reg_best = grid_search_cv_resample(X_train, y_train, log_reg, params, cv=5, resampler=rand_under_sampler, metric='accuracy', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy\t 0.8395698808353359\n",
      "\n",
      "Class\t\t 0\t\t1\n",
      "Precision\t 1.000\t\t0.010\n",
      "Recall\t\t 0.839\t\t0.968\n",
      "f1\t\t 0.913\t\t0.020\n",
      "Support\t\t 39804.000\t68.800\n"
     ]
    }
   ],
   "source": [
    "print_metrics_resample(X_train, y_train, log_reg_best, cv=5, resampler=rand_under_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params {'C': 0.1, 'degree': 1, 'kernel': 'poly'}\n",
      "accuracy score: 0.9993679906113628\n",
      "Time: 114.50580072402954\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "svm_rbf = SVC()\n",
    "\n",
    "# Tuning kernel, C, and degree for polynomial kernel\n",
    "params = [{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}, {'kernel': ['poly'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'degree': [0, 1, 2, 3, 4, 5]}]\n",
    "svm_rbf_best = grid_search_cv_resample(X_train, y_train, svm_rbf, params, cv=5, resampler=rand_under_sampler, metric='accuracy', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy\t 0.9993679906113628\n",
      "\n",
      "Class\t\t 0\t\t1\n",
      "Precision\t 1.000\t\t0.821\n",
      "Recall\t\t 1.000\t\t0.817\n",
      "f1\t\t 1.000\t\t0.817\n",
      "Support\t\t 39804.000\t68.800\n"
     ]
    }
   ],
   "source": [
    "print_metrics_resample(X_train, y_train, svm_rbf_best, cv=5, resampler=rand_under_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params {'n_neighbors': 10}\n",
      "accuracy score: 0.9896169799889126\n",
      "Time: 95.65626168251038\n"
     ]
    }
   ],
   "source": [
    "# knn\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Tuning k\n",
    "params = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "knn_best = grid_search_cv_resample(X_train, y_train, knn, params, cv=5, resampler=rand_under_sampler, metric='accuracy', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy\t 0.9896169799889126\n",
      "\n",
      "Class\t\t 0\t\t1\n",
      "Precision\t 1.000\t\t0.130\n",
      "Recall\t\t 0.990\t\t0.881\n",
      "f1\t\t 0.995\t\t0.227\n",
      "Support\t\t 39804.000\t68.800\n"
     ]
    }
   ],
   "source": [
    "print_metrics_resample(X_train, y_train, knn_best, cv=5, resampler=rand_under_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params {'n_estimators': 100}\n",
      "accuracy score: 0.9718605195844721\n",
      "Time: 13.941757678985596\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rand_forest = RandomForestClassifier()\n",
    "\n",
    "# Tuning number of trees\n",
    "params = {'n_estimators': [1, 10, 100, 500]}\n",
    "rand_forest_best = grid_search_cv_resample(X_train, y_train, rand_forest, params, cv=5, resampler=rand_under_sampler, metric='accuracy', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy\t 0.9705563580550027\n",
      "\n",
      "Class\t\t 0\t\t1\n",
      "Precision\t 1.000\t\t0.051\n",
      "Recall\t\t 0.971\t\t0.913\n",
      "f1\t\t 0.985\t\t0.097\n",
      "Support\t\t 39804.000\t68.800\n"
     ]
    }
   ],
   "source": [
    "print_metrics_resample(X_train, y_train, rand_forest_best, cv=5, resampler=rand_under_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression logistique semble pouvoir détecter le plus de fraudes avec un recall de 0.96 pour la classe 1. \n",
    "\n",
    "On peut évaluer son pouvoir de généralisation sur l'ensemble de test original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_amount</th>\n",
       "      <th>norm_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>688.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>-0.126195</td>\n",
       "      <td>-2.444678</td>\n",
       "      <td>1.901810</td>\n",
       "      <td>-3.638969</td>\n",
       "      <td>2.121004</td>\n",
       "      <td>-1.656680</td>\n",
       "      <td>-0.696165</td>\n",
       "      <td>-2.959041</td>\n",
       "      <td>0.188744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211573</td>\n",
       "      <td>0.440964</td>\n",
       "      <td>-0.010492</td>\n",
       "      <td>-0.019214</td>\n",
       "      <td>-0.028656</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.025031</td>\n",
       "      <td>0.105320</td>\n",
       "      <td>0.059413</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.899612</td>\n",
       "      <td>1.020022</td>\n",
       "      <td>5.697432</td>\n",
       "      <td>3.790652</td>\n",
       "      <td>6.336778</td>\n",
       "      <td>3.284833</td>\n",
       "      <td>4.299497</td>\n",
       "      <td>1.790023</td>\n",
       "      <td>6.034797</td>\n",
       "      <td>5.425015</td>\n",
       "      <td>...</td>\n",
       "      <td>1.097614</td>\n",
       "      <td>3.184309</td>\n",
       "      <td>1.287886</td>\n",
       "      <td>1.312333</td>\n",
       "      <td>0.569880</td>\n",
       "      <td>0.683639</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>1.051400</td>\n",
       "      <td>0.405133</td>\n",
       "      <td>0.500364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.353229</td>\n",
       "      <td>-1.992624</td>\n",
       "      <td>-30.552380</td>\n",
       "      <td>-8.402154</td>\n",
       "      <td>-31.103685</td>\n",
       "      <td>-5.231828</td>\n",
       "      <td>-22.105532</td>\n",
       "      <td>-5.773192</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.128186</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-19.254328</td>\n",
       "      <td>-2.028024</td>\n",
       "      <td>-4.781606</td>\n",
       "      <td>-1.329589</td>\n",
       "      <td>-7.263482</td>\n",
       "      <td>-1.552593</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.346083</td>\n",
       "      <td>-1.039764</td>\n",
       "      <td>-2.870879</td>\n",
       "      <td>-0.164501</td>\n",
       "      <td>-5.181646</td>\n",
       "      <td>-0.324236</td>\n",
       "      <td>-1.864221</td>\n",
       "      <td>-1.556634</td>\n",
       "      <td>-3.152655</td>\n",
       "      <td>-0.208914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195058</td>\n",
       "      <td>-0.166346</td>\n",
       "      <td>-0.524324</td>\n",
       "      <td>-0.254264</td>\n",
       "      <td>-0.374835</td>\n",
       "      <td>-0.314790</td>\n",
       "      <td>-0.278314</td>\n",
       "      <td>-0.062918</td>\n",
       "      <td>-0.051460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.277386</td>\n",
       "      <td>-0.233224</td>\n",
       "      <td>-0.834366</td>\n",
       "      <td>0.945278</td>\n",
       "      <td>-1.417632</td>\n",
       "      <td>1.084360</td>\n",
       "      <td>-0.483200</td>\n",
       "      <td>-0.678117</td>\n",
       "      <td>-0.744855</td>\n",
       "      <td>0.178904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048539</td>\n",
       "      <td>0.161919</td>\n",
       "      <td>0.039776</td>\n",
       "      <td>-0.033762</td>\n",
       "      <td>0.024644</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>-0.020932</td>\n",
       "      <td>0.058968</td>\n",
       "      <td>0.040077</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.046539</td>\n",
       "      <td>0.836434</td>\n",
       "      <td>1.048206</td>\n",
       "      <td>2.825485</td>\n",
       "      <td>0.193490</td>\n",
       "      <td>4.173330</td>\n",
       "      <td>0.397083</td>\n",
       "      <td>0.093819</td>\n",
       "      <td>0.229025</td>\n",
       "      <td>0.954627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483930</td>\n",
       "      <td>0.717166</td>\n",
       "      <td>0.610616</td>\n",
       "      <td>0.202751</td>\n",
       "      <td>0.378893</td>\n",
       "      <td>0.382632</td>\n",
       "      <td>0.356523</td>\n",
       "      <td>0.468181</td>\n",
       "      <td>0.227415</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.146182</td>\n",
       "      <td>1.608807</td>\n",
       "      <td>2.328167</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>3.177069</td>\n",
       "      <td>12.114672</td>\n",
       "      <td>11.095089</td>\n",
       "      <td>6.474115</td>\n",
       "      <td>5.431271</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>...</td>\n",
       "      <td>11.059004</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>5.466230</td>\n",
       "      <td>3.508563</td>\n",
       "      <td>2.208209</td>\n",
       "      <td>1.246604</td>\n",
       "      <td>3.052358</td>\n",
       "      <td>1.779364</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       norm_amount   norm_time          V1          V2          V3  \\\n",
       "count   688.000000  688.000000  688.000000  688.000000  688.000000   \n",
       "mean      0.060000   -0.126195   -2.444678    1.901810   -3.638969   \n",
       "std       0.899612    1.020022    5.697432    3.790652    6.336778   \n",
       "min      -0.353229   -1.992624  -30.552380   -8.402154  -31.103685   \n",
       "25%      -0.346083   -1.039764   -2.870879   -0.164501   -5.181646   \n",
       "50%      -0.277386   -0.233224   -0.834366    0.945278   -1.417632   \n",
       "75%       0.046539    0.836434    1.048206    2.825485    0.193490   \n",
       "max       8.146182    1.608807    2.328167   22.057729    3.177069   \n",
       "\n",
       "               V4          V5          V6          V7          V8  ...  \\\n",
       "count  688.000000  688.000000  688.000000  688.000000  688.000000  ...   \n",
       "mean     2.121004   -1.656680   -0.696165   -2.959041    0.188744  ...   \n",
       "std      3.284833    4.299497    1.790023    6.034797    5.425015  ...   \n",
       "min     -5.231828  -22.105532   -5.773192  -43.557242  -41.044261  ...   \n",
       "25%     -0.324236   -1.864221   -1.556634   -3.152655   -0.208914  ...   \n",
       "50%      1.084360   -0.483200   -0.678117   -0.744855    0.178904  ...   \n",
       "75%      4.173330    0.397083    0.093819    0.229025    0.954627  ...   \n",
       "max     12.114672   11.095089    6.474115    5.431271   20.007208  ...   \n",
       "\n",
       "              V20         V21         V22         V23         V24         V25  \\\n",
       "count  688.000000  688.000000  688.000000  688.000000  688.000000  688.000000   \n",
       "mean     0.211573    0.440964   -0.010492   -0.019214   -0.028656    0.018229   \n",
       "std      1.097614    3.184309    1.287886    1.312333    0.569880    0.683639   \n",
       "min     -4.128186  -22.797604   -8.887017  -19.254328   -2.028024   -4.781606   \n",
       "25%     -0.195058   -0.166346   -0.524324   -0.254264   -0.374835   -0.314790   \n",
       "50%      0.048539    0.161919    0.039776   -0.033762    0.024644    0.064023   \n",
       "75%      0.483930    0.717166    0.610616    0.202751    0.378893    0.382632   \n",
       "max     11.059004   27.202839    8.361985    5.466230    3.508563    2.208209   \n",
       "\n",
       "              V26         V27         V28       Class  \n",
       "count  688.000000  688.000000  688.000000  688.000000  \n",
       "mean     0.025031    0.105320    0.059413    0.500000  \n",
       "std      0.463300    1.051400    0.405133    0.500364  \n",
       "min     -1.329589   -7.263482   -1.552593    0.000000  \n",
       "25%     -0.278314   -0.062918   -0.051460    0.000000  \n",
       "50%     -0.020932    0.058968    0.040077    0.500000  \n",
       "75%      0.356523    0.468181    0.227415    1.000000  \n",
       "max      1.246604    3.052358    1.779364    1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New under-sampled df to retrain with more data\n",
    "\n",
    "# Select a random sample of non-fraud exemples\n",
    "fraud_df = train_df[train_df['Class']==1]\n",
    "non_fraud_df = train_df[train_df['Class']==0]\n",
    "rdm_non_fraud_df = non_fraud_df.sample(n=fraud_df.shape[0], random_state=0)\n",
    "\n",
    "# New balanced df\n",
    "under_sample_df = pd.concat([fraud_df, rdm_non_fraud_df])\n",
    "under_sample_df = under_sample_df.sample(frac=1, random_state=0)\n",
    "\n",
    "# Split random undersample (rus) data \n",
    "X_train_rus, y_train_rus = under_sample_df.drop('Class', axis=1), under_sample_df['Class']\n",
    "\n",
    "under_sample_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance de régression logistique entraînée avec sub-sampling aléatoire sur l'ensemble test \n",
      "\n",
      "Accuracy: 0.8278267382933652\n",
      "\n",
      "true positives\tfalse positives\tfalse negatives\ttrue negatives\n",
      "70588\t\t14707\t\t4\t\t144\n",
      "\n",
      "Class\t\t 0\t\t1\n",
      "Precision\t 1.00\t\t0.01\n",
      "recall\t\t 0.83\t\t0.97\n",
      "f1\t\t 0.91\t\t0.02\n",
      "support\t\t 85295.00\t148.00\n"
     ]
    }
   ],
   "source": [
    "log_reg_best.fit(X_train_rus, y_train_rus)\n",
    "predictions = log_reg_best.predict(X_test)\n",
    "print_metrics(y_test, predictions, 'Performance de régression logistique entraînée avec sub-sampling aléatoire sur l\\'ensemble test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc Auc score régression logistique: 0.9002739300646564\n"
     ]
    }
   ],
   "source": [
    "roc_auc_reg = roc_auc_score(y_test, predictions)\n",
    "print('Roc Auc score régression logistique:', roc_auc_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "decision_function = log_reg_best.decision_function(X_test)\n",
    "\n",
    "# ROC curves\n",
    "log_reg_fpr, log_reg_tpr, log_reg_thresholds = roc_curve(y_test, decision_function)\n",
    "\n",
    "plt.title('courbe ROC')\n",
    "plt.plot(log_reg_fpr, log_reg_tpr, label='Régression logistique')\n",
    "plt.xlabel('taux de faux positifs')\n",
    "plt.ylabel('taux de vrais positifs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curves\n",
    "log_reg_precision, log_reg_recall, log_reg_thresholds = precision_recall_curve(y_test, decision_function)\n",
    "\n",
    "plt.title('Courbe precision-recall')\n",
    "plt.plot(log_reg_precision, log_reg_recall, label='Régression logistique')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut s'amuser à observer l'effet du threshold pour la classification. Peut-être peut-on gagné un peu d'accuracy sans trop diminuer le recall pour la classe 1.\n",
    "\n",
    "Pour ce faire, créons un ensemble d'entraînement et un ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold= 0.4\n",
      "Accuracy= 0.001872593424856337\n",
      "true positives\tfalse positives\tfalse negatives\ttrue negatives\n",
      "12\t\t85283\t\t0\t\t148\n",
      "Precision= 0.0017323922229635612\n",
      "Recall= 1.0\n",
      "\n",
      "Threshold= 0.5\n",
      "Accuracy= 0.8440129677094671\n",
      "true positives\tfalse positives\tfalse negatives\ttrue negatives\n",
      "71974\t\t13321\t\t7\t\t141\n",
      "Precision= 0.010473926608230575\n",
      "Recall= 0.9527027027027027\n",
      "\n",
      "Threshold= 0.6\n",
      "Accuracy= 0.9885186615638496\n",
      "true positives\tfalse positives\tfalse negatives\ttrue negatives\n",
      "84336\t\t959\t\t22\t\t126\n",
      "Precision= 0.11612903225806452\n",
      "Recall= 0.8513513513513513\n",
      "\n",
      "Threshold= 0.7\n",
      "Accuracy= 0.9988530365272755\n",
      "true positives\tfalse positives\tfalse negatives\ttrue negatives\n",
      "85224\t\t71\t\t27\t\t121\n",
      "Precision= 0.6302083333333334\n",
      "Recall= 0.8175675675675675\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(40, 71, 10)/100 # For fraud\n",
    "\n",
    "predictions_proba = log_reg_best.predict_proba(X_test)[:,-1]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Prediction\n",
    "    predictions = (predictions_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Quality \n",
    "    confusion_m = confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = confusion_m.ravel()\n",
    "    print('\\nThreshold=', threshold)\n",
    "    print('Accuracy=', accuracy_score(y_test, predictions))\n",
    "    print('true positives\\tfalse positives\\tfalse negatives\\ttrue negatives')\n",
    "    print('%.0f\\t\\t%.0f\\t\\t%.0f\\t\\t%.0f'% (tn, fp, fn, tp))\n",
    "    print('Precision=', precision_score(y_test, predictions))\n",
    "    print('Recall=', recall_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dépendemment du contexte, on pourrait choisir différents thresholds. Par exemple, si une banque veut minimiser le taux de faux positifs pour ne pas déranger ses clients inutliement, et ce, au détriment du pouvoir de détection de fraudes, alors peut-être qu'on pourrait choisir un threshold de 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling cluster\n",
    "\n",
    "Une autre façon de ne sélectionner qu'une partie des exemples de la classee majoritaire pour balancer les données est l'undersampling par centroïdes. Il s'agit de performer K-Mean sur l'ensemble majoritaire pour sélectionner les exemples à conserver. Cette méthode s'assure alors de sélectioner des éléments moyens et typiques qui devraient bien représenter la classe majoritaire.\n",
    "\n",
    "Perform under-sampling by generating centroids based on clustering methods.\n",
    "\n",
    "\"Method that under samples the majority class by replacing a cluster of majority samples by the cluster centroid of a KMeans algorithm. This algorithm keeps N majority samples by fitting the KMeans algorithm with N cluster to the majority class and using the coordinates of the N cluster centroids as the new majority samples.\" Documentation\n",
    "Try RandomUnderSampler maybe ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampler = ClusterCentroids(sampling_strategy='majority', random_state=0)\n",
    "X_resampled, y_resampled = under_sampler.fit_resample(X_train, y_train) # This takes so long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cluster = LogisticRegression()\n",
    "params = {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "log_reg_cluster = GridSearchCV(log_reg_cluster, params, scoring='accuracy', cv=5, verbose=1)\n",
    "log_reg_cluster.fit(X_resampled, y_resampled)\n",
    "print(log_reg_cluster.best_params_)\n",
    "print('Meilleur résultat (moyenne des résultats de validation croisée):', log_reg_cluster.best_score_)\n",
    "log_reg_cluster_best = log_reg_cluster.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_reg_cluster.predict(X_test)\n",
    "print_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "Génère des exemples synthétiques entre les exemples de la classe minoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_os = SMOTE(sampling_strategy='minority', random_state=0)\n",
    "X_resampled, y_resampled = smote_os.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_smote = LogisticRegression()\n",
    "\n",
    "params = {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "log_reg_smote = GridSearchCV(log_reg_smote, params, scoring='accuracy', cv=5, verbose=1)\n",
    "log_reg_smote.fit(X_resampled, y_resampled)\n",
    "print(log_reg_smote.best_params_)\n",
    "print('Meilleur résultat (moyenne des résultats de validation croisée):', log_reg_smote.best_score_)\n",
    "log_reg_smote_best = log_reg_smote.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_reg_smote_best.predict(X_test)\n",
    "print_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "[Addressing the Curse of Imbalanced Training Sets: One-Sided Selection](https://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf)\n",
    "\n",
    "[Credit Fraud Detector (notebook kaggle)](https://www.kaggle.com/kernels/scriptcontent/16695845/download)\n",
    "\n",
    "[Toward data science article](https://towardsdatascience.com/detecting-financial-fraud-using-machine-learning-three-ways-of-winning-the-war-against-imbalanced-a03f8815cce9)\n",
    "\n",
    "[Evaluation Metrics, ROC-Curves and imbalanced datasets](http://www.davidsbatista.net/blog/2018/08/19/NLP_Metrics/)\n",
    "\n",
    "[Subsampling and Oversampling Wikipedia](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis#Undersampling_techniques_for_classification_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
